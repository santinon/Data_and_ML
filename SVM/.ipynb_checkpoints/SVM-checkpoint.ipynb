{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)   \n",
    "\n",
    "a powerful supervised machine learning model used for classification.  \n",
    "An SVM makes classifications by defining a **decision boundary** and then seeing what side of the boundary an unclassified point falls on.\n",
    "\n",
    "- when the data has two features, the decision boundary is a line\n",
    "-  If there are three features, the decision boundary is a plane \n",
    "- more than 3 dimensions/ features,  the decision boundary is called \"separating hyperplane\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **support vectors** are the points in the training set closest to the decision boundary ( If you are using n features, there are at least n+1 support vectors).  \n",
    "\n",
    "\n",
    "The distance between a support vector and the decision boundary is called the **margin**. We want to make the margin as large as possible.  \n",
    "\n",
    "\n",
    "Because the support vectors are so critical in defining the decision boundary, many of the other training points can be ignored. This is one of the advantages of SVMs.  \n",
    "Many supervised machine learning algorithms use every training point in order to make a prediction, even though many of those training points aren’t relevant. SVMs are fast because they only use the support vectors!  \n",
    "\n",
    "________________________\n",
    "**Outliers**  \n",
    "SVMs have a parameter **C** that determines how much error the SVM will allow for.  \n",
    "- If **C** is large, SVM has a hard margin — it won’t allow for misclassifications and as a result, the margin could be fairly small. \n",
    "   - If C is too large, model risks of overfitting (it relies too heavily on the training data, including the outliers)  \n",
    "   \n",
    "\n",
    "- if **C** gets too small, the model risks of underfitting.  \n",
    "   \n",
    "> Python :  \n",
    "classifier = SVC(**C** = 0.01)  \n",
    "(C will depend on the data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikit-learn\n",
    "(calculating the parameters of the best decision boundary is a fairly complex optimization problem. Luckily, Python’s scikit-learn library has implemented an SVM that will do this for us)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.5833333333333334\n",
      "0.5666666666666667\n",
      "0.5666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Makes concentric circles\n",
    "points, labels = make_circles(n_samples=300, factor=.2, noise=.05, random_state = 1)\n",
    "\n",
    "\n",
    "training_data, validation_data, training_labels, validation_labels = train_test_split(points,\n",
    "                                                                                      labels,\n",
    "                                                                                      train_size = 0.8, \n",
    "                                                                                      test_size = 0.2,\n",
    "                                                                                      random_state = 100)\n",
    "# \"degree = 2\": we want to project on a plane of dimensions 2 in fine\n",
    "classifier  = SVC(kernel = \"poly\", degree = 2)    # polynomial kernel transforms points into three dimensions\n",
    "classifier1 = SVC(kernel = \"poly\")                # polynomial kernel transforms points into three dimensions\n",
    "classifier2 = SVC(kernel = \"linear\", degree = 2)  # linear boundary\n",
    "classifier3 = SVC(kernel = \"linear\")              # linear boundary\n",
    "classifier.fit(training_data, training_labels)\n",
    "classifier1.fit(training_data, training_labels)\n",
    "classifier2.fit(training_data, training_labels) \n",
    "classifier3.fit(training_data, training_labels)\n",
    "print(classifier.score(validation_data, validation_labels))\n",
    "print(classifier1.score(validation_data, validation_labels))\n",
    "print(classifier2.score(validation_data, validation_labels))\n",
    "print(classifier3.score(validation_data, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5666666666666667\n",
      "[0.31860062 0.11705731]\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Makes concentric circles\n",
    "points, labels = make_circles(n_samples=300, factor=.2, noise=.05, random_state = 1)\n",
    "\n",
    "#Makes training set and validation set.\n",
    "training_data, validation_data, training_labels, validation_labels = train_test_split(points,\n",
    "                                                                                      labels,\n",
    "                                                                                      train_size = 0.8,\n",
    "                                                                                      test_size = 0.2,\n",
    "                                                                                      random_state = 100)\n",
    "\n",
    "# linear kernel \n",
    "classifier = SVC(kernel = \"linear\", random_state = 1)\n",
    "classifier.fit(training_data, training_labels)\n",
    "print(classifier.score(validation_data, validation_labels))\n",
    "print(training_data[0])\n",
    "\n",
    "\n",
    "\n",
    "# polynomial kernel (detailled afterwards just below), transforms points into three dimensions\n",
    "classifier1 = SVC(kernel = \"poly\", degree = 2)  # we want to project on a plane of dimensions 2 in fine\n",
    "classifier1.fit(training_data, training_labels)\n",
    "print(classifier1.score(validation_data, validation_labels))\n",
    "\n",
    "# Important :\n",
    "# what the polynomial kernel is doing, is detailed below : \n",
    "new_training = [[2 ** 0.5 * pt[0] * pt[1], pt[0] ** 2, pt[1] ** 2] for pt in training_data]\n",
    "new_validation = [[2 ** 0.5 * pt[0] * pt[1], pt[0] ** 2, pt[1] ** 2] for pt in validation_data]\n",
    "\n",
    "classifier.fit(new_training, training_labels)\n",
    "print(classifier.score(new_validation, validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial Bias Function Kernel\n",
    "The most commonly used kernel in SVMs is a radial basis function (**rbf**) kernel. This is the default kernel used in scikit-learn’s SVC object. If you don’t specifically set the kernel to \"linear\", \"poly\" the SVC object will use an rbf kernel.  \n",
    "rbf kernel transforms points into infinite dimensions\n",
    "\n",
    "> Python:  \n",
    "classifier = SVC(kernel = \"rbf\", **gamma** = 0.5, C = 2)\n",
    "\n",
    "**gamma** is similar to the C parameter. You can essentially tune the model to be more or less sensitive to the training data. \n",
    "- Higher gamma, say 100, could result in overfitting (will put more importance on the training data).  \n",
    "\n",
    "\n",
    "- Lower gamma like 0.01, can result in underfitting (makes the points in the training data less relevant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma 52\n",
      "1.0\n",
      "0.7222222222222222\n",
      "\n",
      "gamma 10\n",
      "1.0\n",
      "0.8333333333333334\n",
      "\n",
      "gamma 1\n",
      "0.9930555555555556\n",
      "0.8888888888888888\n",
      "\n",
      "gamma 0.1\n",
      "0.8611111111111112\n",
      "0.7777777777777778\n",
      "\n",
      "gamma 0.01\n",
      "0.7986111111111112\n",
      "0.7222222222222222\n"
     ]
    }
   ],
   "source": [
    "from data import points, labels\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "training_data, validation_data, training_labels, validation_labels = train_test_split(points,\n",
    "                                                                                      labels, \n",
    "                                                                                      train_size = 0.8, \n",
    "                                                                                      test_size = 0.2, \n",
    "                                                                                      random_state = 100)\n",
    "\n",
    "print(\"gamma 52\")\n",
    "classifier = SVC(kernel = \"rbf\", gamma = 52) # rbf kernel transforms points into infinite dimensions\n",
    "classifier.fit(training_data, training_labels)\n",
    "print(classifier.score(training_data, training_labels))\n",
    "print(classifier.score(validation_data, validation_labels))\n",
    "\n",
    "print(\"\\ngamma 10\")\n",
    "classifier = SVC(kernel = \"rbf\", gamma = 10)\n",
    "classifier.fit(training_data, training_labels)\n",
    "print(classifier.score(training_data, training_labels))\n",
    "print(classifier.score(validation_data, validation_labels))\n",
    "\n",
    "print(\"\\ngamma 1\")\n",
    "classifier = SVC(kernel = \"rbf\", gamma = 1)\n",
    "classifier.fit(training_data, training_labels)\n",
    "print(classifier.score(training_data, training_labels))\n",
    "print(classifier.score(validation_data, validation_labels))\n",
    "\n",
    "print(\"\\ngamma 0.1\")\n",
    "classifier = SVC(kernel = \"rbf\", gamma = 0.1)\n",
    "classifier.fit(training_data, training_labels)\n",
    "print(classifier.score(training_data, training_labels))\n",
    "print(classifier.score(validation_data, validation_labels))\n",
    "\n",
    "print(\"\\ngamma 0.01\")\n",
    "classifier = SVC(kernel = \"rbf\", gamma = 0.01) # rbf kernel transforms points into infinite dimensions\n",
    "classifier.fit(training_data, training_labels)\n",
    "print(classifier.score(training_data, training_labels))\n",
    "print(classifier.score(validation_data, validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '\\xef'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-466c78d588a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msvm_visualization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdraw_boundary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mplayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maaron_judge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# ax = axes of your graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git_hub/big_data/SVM/players.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0maaron_judge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"aaron_judge.p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mjose_altuve\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"jose_altuve.p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdavid_ortiz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"david_ortiz.p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '\\xef'."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from svm_visualization import draw_boundary\n",
    "from players import aaron_judge\n",
    "\n",
    "fig, ax = plt.subplots() # ax = axes of your graph\n",
    "\n",
    "# 1/ Create the labels\n",
    "\n",
    "#print(aaron_judge.columns)\n",
    "#print(aaron_judge.description.unique())\n",
    "#print(aaron_judge.type.unique())  # ['S' 'B' 'X']\n",
    "\n",
    "# change every 'S' to a 1 and every 'B' to a 0\n",
    "aaron_judge['type'] = aaron_judge['type'].map({'S': 1, 'B': 0})\n",
    "#print(aaron_judge['type'])  #  check\n",
    "\n",
    "\n",
    "# 2/ Plotting the pitches\n",
    "\n",
    "#print(aaron_judge['plate_x'], aaron_judge['plate_z'])\n",
    "\n",
    "# remove every row that has a NaN in any of those columns\n",
    "aaron_judge = aaron_judge.dropna(subset = ['type', 'plate_x', 'plate_z'])\n",
    "\n",
    "# We have points to plot using Matplotlib\n",
    "plt.scatter(x = aaron_judge['plate_x'], y = aaron_judge['plate_z'], c = aaron_judge['type'], cmap = plt.cm.coolwarm, alpha = 0.5)\n",
    "# To color the points correctly, the parameter c should be the type column\n",
    "# To make the strikes red and the balls blue, set the cmap parameter to plt.cm.coolwarm\n",
    "# To make the points slightly transparent, set the alpha parameter to 0.25\n",
    "\n",
    "\n",
    "# 3/ Building the SVM \n",
    "\n",
    "training_set, validation_set = train_test_split(aaron_judge, random_state = 1)\n",
    "classifier = SVC(kernel = \"rbf\")\n",
    "classifier.fit(training_set[['plate_x', 'plate_z']], training_set['type']) # The trained SVM\n",
    "\n",
    "#  visualize the SVM, call the draw_boundary function. This is a function that we wrote ourselves - you won’t find it in scikit-learn\n",
    "draw_boundary(ax, classifier)\n",
    "# The axes of your graph. For us, is the ax variable that we defined at the top of your code.\n",
    "# The trained SVM. For us, this is classifier. Make sure you’ve called .fit() before trying to visualize the decision boundary.\n",
    "\n",
    "plt.show()\n",
    "print(classifier.score(validation_set[['plate_x', 'plate_z']], validation_set['type']))\n",
    "\n",
    "\n",
    "# 4/ Optimizing the SVM\n",
    "\n",
    "classifier = SVC(kernel = \"rbf\", C = 100, gamma = 100)\n",
    "classifier.fit(training_set[['plate_x', 'plate_z']], training_set['type']) # The trained SVM\n",
    "draw_boundary(ax, classifier)\n",
    "plt.show()\n",
    "print(classifier.score(validation_set[['plate_x', 'plate_z']], validation_set['type']))\n",
    "\n",
    "\n",
    "\n",
    "def best_para(training_set, validation_set, maximal):\n",
    "  largest = {'value': 0, \"C\": 1, 'gamma': 1}\n",
    "  for gamma in range(1,maximal):\n",
    "    for C in range(1,maximal):\n",
    "      classifier = SVC(kernel = \"rbf\", C = C, gamma = gamma)\n",
    "      classifier.fit(training_set[['plate_x', 'plate_z']], training_set['type']) # The trained SVM\n",
    "      score = classifier.score(validation_set[['plate_x', 'plate_z']], validation_set['type'])\n",
    "      if largest['value'] < score:\n",
    "        largest['value'] = score\n",
    "        largest['C'] = C\n",
    "        largest['gamma'] = gamma\n",
    "  draw_boundary(ax, classifier)\n",
    "  plt.show()  \n",
    "  print(largest)\n",
    "\n",
    "best_para(training_set, validation_set, 10)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "- SVMs are supervised machine learning models used for classification.  \n",
    "    \n",
    "    \n",
    "- An SVM uses support vectors to define a decision boundary. Classifications are made by comparing unlabeled points to that decision boundary.  \n",
    "    \n",
    "\n",
    "- Support vectors are the points of each class closest to the decision boundary. The distance between the support vectors and the decision boundary is called the margin.  \n",
    "    \n",
    "    \n",
    "- SVMs attempt to create the largest margin possible while staying within an acceptable amount of error.\n",
    "    The C parameter controls how much error is allowed. A large C allows for little error and creates a hard margin. A small C allows for more error and creates a soft margin.  \n",
    "    \n",
    "    \n",
    "- SVMs use kernels to classify points that aren’t linearly separable.  \n",
    "    \n",
    "    \n",
    "- Kernels transform points into higher dimensional space. A polynomial kernel transforms points into three dimensions while an rbf kernel transforms points into infinite dimensions.  \n",
    "    \n",
    "    \n",
    "- An rbf kernel has a gamma parameter. If gamma is large, the training data is more relevant, and as a result overfitting can occur.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
